{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '../../data/train/'\n",
    "outlier_path = '../../data/647outlier/'\n",
    "dicescore_csv = \"../../data/cp-id4-37-sb-coef.csv\"\n",
    "BATCHSIZE = 12\n",
    "EPOCH_NUM = 60\n",
    "MODEL_NAME = '647unet-{epoch:02d}-{val_loss:.4f}-id7.h5'\n",
    "BASE_LR=1e-4\n",
    "MAX_LR=6e-4\n",
    "STEP_SIZE=3000.\n",
    "MODE='triangular2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/train/16001_sat.jpg'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7e95dbabd0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlier_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_sat.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmove_outlier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-7e95dbabd0e7>\u001b[0m in \u001b[0;36mmove_outlier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimg_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_sat.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchoose_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/train/16001_sat.jpg'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "\n",
    "path_to_train = '../../data/train/'\n",
    "outlier_path = '../../data/647outlier/'\n",
    "dicescore_csv = \"../../data/cp-id4-37-sb-coef.csv\"\n",
    "\n",
    "def move_outlier():\n",
    "    csv_file = open(dicescore_csv, 'r')\n",
    "    all_lines = csv_file.readlines()\n",
    "    for line in all_lines[1:648]:\n",
    "        list = line.split(',')\n",
    "        img_id = list[0]\n",
    "        os.remove(path_to_train + img_id + '_sat.jpg')\n",
    "\n",
    "def choose_normal():\n",
    "    normal_paths = glob(path_to_train + \"/*_sat.jpg\")\n",
    "    normal_imgs = np.random.choice(normal_paths, 3*647)\n",
    "    for path in normal_imgs:\n",
    "        img = imread(path)\n",
    "        img_basename = os.path.basename(path)\n",
    "        img_id = os.path.splitext(img_basename)[0][:-len('_sat')]\n",
    "        imsave(outlier_path+img_id+'_sat.jpg', img)\n",
    "\n",
    "move_outlier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/train/53717_sat.jpg', '../../data/train/12320_sat.jpg', '../../data/train/53640_sat.jpg', '../../data/train/12480_sat.jpg', '../../data/train/27456_sat.jpg', '../../data/train/40397_sat.jpg', '../../data/train/34417_sat.jpg', '../../data/train/24192_sat.jpg', '../../data/train/42547_sat.jpg', '../../data/train/18984_sat.jpg']\n",
      "['../../data/train/52857_msk.png', '../../data/train/5009_msk.png', '../../data/train/29412_msk.png', '../../data/train/50596_msk.png', '../../data/train/8066_msk.png', '../../data/train/11225_msk.png', '../../data/train/43068_msk.png', '../../data/train/48526_msk.png', '../../data/train/47933_msk.png', '../../data/train/15692_msk.png']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "path_to_train = '../../data/train/'\n",
    "glob_train_imgs = os.path.join(path_to_train, '*_sat.jpg')\n",
    "glob_train_masks = os.path.join(path_to_train, '*_msk.png')\n",
    "\n",
    "train_img_paths = glob(glob_train_imgs)\n",
    "train_mask_paths = glob(glob_train_masks)\n",
    "print(train_img_paths[:10])\n",
    "print(train_mask_paths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def get_img_id(img_path):\n",
    "  img_basename = os.path.basename(img_path)\n",
    "  img_id = os.path.splitext(img_basename)[0][:-len('_sat')]\n",
    "  return img_id\n",
    "\n",
    "def img_gen(img_paths, img_size=(512, 512)):\n",
    "  #Iterate over all image paths\n",
    "  for img_path in img_paths:\n",
    "    img_id = get_img_id(img_path)\n",
    "    mask_path = os.path.join(path_to_train,img_id+'_msk.png')\n",
    "    \n",
    "    img = imread(img_path)/255\n",
    "    mask = rgb2gray(imread(mask_path))\n",
    "    \n",
    "#     img = resize(img, img_size, preserve_range = True)\n",
    "#     mask = resize(mask, img_size, mode='constant', preserve_range = True)\n",
    "    \n",
    "    mask = (mask>=0.5).astype(float)\n",
    "    mask = np.reshape(mask, (512,512,1))\n",
    "    yield img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Keras takes its input in batches \n",
    "# (i.e. a batch size of 32 would correspond to 32 images and 32 masks from the generator)\n",
    "# The generator should run forever\n",
    "def image_batch_generator(img_paths, batchsize=12):\n",
    "    while True:\n",
    "        ig = img_gen(img_paths)\n",
    "        batch_img, batch_mask = [], []\n",
    "        \n",
    "        for img, mask in ig:\n",
    "            # Add the image and mask to the batch; augument on the fly\n",
    "            batch_img.append(img)\n",
    "            batch_mask.append(mask)\n",
    "            batch_img.append(img[:,::-1])\n",
    "            batch_mask.append(mask[:,::-1])\n",
    "            batch_img.append(img[::-1,:])\n",
    "            batch_mask.append(mask[::-1,:])\n",
    "            # If we've reached our batchsize, yield the batch and reset\n",
    "            if len(batch_img) == batchsize:\n",
    "                shuffle_idx = np.arange(len(batch_img))\n",
    "                np.random.shuffle(shuffle_idx)\n",
    "                batch_img = [batch_img[i] for i in shuffle_idx] \n",
    "                batch_mask = [batch_mask[i] for i in shuffle_idx] \n",
    "                yield np.stack(batch_img, axis=0), np.stack(batch_mask, axis=0)\n",
    "                batch_img, batch_mask = [], []\n",
    "        \n",
    "        # If we have an nonempty batch left, yield it out and reset\n",
    "        if len(batch_img) != 0:\n",
    "            shuffle_idx = np.arange(len(batch_img))\n",
    "            np.random.shuffle(shuffle_idx)\n",
    "            batch_img = [batch_img[i] for i in shuffle_idx] \n",
    "            batch_mask = [batch_mask[i] for i in shuffle_idx] \n",
    "            yield np.stack(batch_img, axis=0), np.stack(batch_mask, axis=0)\n",
    "            batch_img, batch_mask = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712\n",
      "1538\n",
      "(12, 512, 512, 3)\n",
      "train steps 726\n",
      "val steps 129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into a train and validation set\n",
    "train_img_paths, val_img_paths = train_test_split(train_img_paths, test_size=0.15)\n",
    "print(len(train_img_paths))\n",
    "print(len(val_img_paths))\n",
    "\n",
    "# Create the train and validation generators\n",
    "traingen = image_batch_generator(train_img_paths, batchsize=BATCHSIZE)\n",
    "valgen = image_batch_generator(val_img_paths, batchsize=BATCHSIZE)\n",
    "print(next(traingen)[0].shape)\n",
    "def calc_steps(data_len, batchsize):\n",
    "    return (data_len + batchsize - 1) // batchsize\n",
    "\n",
    "# Calculate the steps per epoch\n",
    "train_steps = calc_steps(len(train_img_paths), BATCHSIZE)\n",
    "val_steps = calc_steps(len(val_img_paths), BATCHSIZE)\n",
    "print(\"train steps {}\".format(train_steps))\n",
    "print(\"val steps {}\".format(val_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512, 512, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 512, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 16) 2320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512, 512, 16) 64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 16) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 32) 4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 256, 32) 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256, 256, 32) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 32) 9248        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 36928       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 128)  147584      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  590080      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2359808     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 256)  524544      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 512)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 256)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  590080      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 128)  131200      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 128)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147584      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 64) 32832       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128, 64) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 32) 8224        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256, 256, 32) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 9248        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 512, 512, 16) 2064        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_5[0][0]         \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 16) 4624        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512, 512, 16) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 512, 512, 16) 2320        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 512, 512, 1)  17          conv2d_22[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,783,665\n",
      "Trainable params: 7,779,633\n",
      "Non-trainable params: 4,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "  \n",
    "# Build U-Net model\n",
    "inputs = Input((512, 512, 3))\n",
    "s = inputs\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "c1 = BatchNormalization()(c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "c1 = BatchNormalization()(c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = BatchNormalization()(c2)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "c2 = BatchNormalization()(c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = BatchNormalization()(c3)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "c3 = BatchNormalization()(c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = BatchNormalization()(c4)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "c4 = BatchNormalization()(c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = BatchNormalization()(c5)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "c5 = BatchNormalization()(c5)\n",
    "p5 = MaxPooling2D(pool_size=(2, 2)) (c5)\n",
    "\n",
    "c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p5)\n",
    "c6 = BatchNormalization()(c6)\n",
    "c6 = Dropout(0.3) (c6)\n",
    "c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "c6 = BatchNormalization()(c6)\n",
    "# 16\n",
    "\n",
    "u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c5])\n",
    "c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "\n",
    "u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c4])\n",
    "c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = Dropout(0.2) (c8)\n",
    "c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "\n",
    "u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c3])\n",
    "c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = Dropout(0.2) (c9)\n",
    "c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "u10 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c9)\n",
    "u10 = concatenate([u10, c2])\n",
    "c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)\n",
    "c10 = Dropout(0.2) (c10)\n",
    "c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)\n",
    "\n",
    "u11 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c10)\n",
    "u11 = concatenate([u11, c1])\n",
    "c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u11)\n",
    "c11 = Dropout(0.2) (c11)\n",
    "c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c11)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "smooth = 1e-9\n",
    "\n",
    "# This is the competition metric implemented using Keras\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * (K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "# We'll construct a Keras Loss that incorporates the DICE score\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return 1. - (2. * intersection + 1.) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.)\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * binary_crossentropy(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)\n",
    "  \n",
    "def ln_dice(y_true, y_pred):\n",
    "  y_true_f = K.flatten(y_true)\n",
    "  y_pred = K.cast(y_pred, 'float32')\n",
    "  y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "  intersection = y_true_f * y_pred_f\n",
    "  score = 2. * (K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "  return K.log(score)\n",
    "\n",
    "def new_bce_dice_loss(y_true, y_pred):\n",
    "  return binary_crossentropy(y_true, y_pred) - ln_dice(y_true, y_pred)\n",
    "  \n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(Adam(1e-4), loss=bce_dice_loss, metrics=[dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 66/726 [=>............................] - ETA: 14:47 - loss: 0.6782 - dice_coef: 0.0699"
     ]
    }
   ],
   "source": [
    "clr = CyclicLR(base_lr=BASE_LR, max_lr=MAX_LR,\n",
    "                                step_size=STEP_SIZE, mode=MODE)\n",
    "checkpointer = ModelCheckpoint(MODEL_NAME, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "    traingen, \n",
    "    steps_per_epoch=train_steps, \n",
    "    epochs=EPOCH_NUM, # Change this to a larger number to train for longer\n",
    "    validation_data=valgen, \n",
    "    validation_steps=val_steps, \n",
    "    verbose=1,\n",
    "    max_queue_size=5,  # Change this number based on memory restrictions\n",
    "    callbacks = [checkpointer]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unet_1)",
   "language": "python",
   "name": "unet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
